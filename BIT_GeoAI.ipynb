{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sxNwd01BpZeGZW2sUaoCjAak_Xk-wTjl",
      "authorship_tag": "ABX9TyNyMlKm6DYRQwyPkuVRGgHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suffergate/Stack/blob/main/BIT_GeoAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remote Sensing Image Change Detection"
      ],
      "metadata": {
        "id": "Rtfdd8Pl22WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcQFUWLf264Z",
        "outputId": "1db4fe72-264b-4395-fe8b-c6cb0eb7d51c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18 S5 Backbone + Prediction Head"
      ],
      "metadata": {
        "id": "EEsYlZQLL6fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBackbone(nn.Module):\n",
        "    \"\"\"Modified ResNet18 backbone for feature extraction\"\"\"\n",
        "    def __init__(self, output_channels=32, backbone='resnet18'):\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet\n",
        "        if backbone == 'resnet18':\n",
        "            self.resnet = models.resnet18(pretrained=True)\n",
        "        else:\n",
        "            raise NotImplementedError('Only ResNet18 is supported now')\n",
        "\n",
        "        # Whether to upsample by 2x at the end\n",
        "        self.if_upsample_2x = True\n",
        "\n",
        "        # Pointwise convolution to reduce channel dimension\n",
        "        self.conv_pred = nn.Conv2d(512, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Replace stride in last two layers to preserve spatial resolution\n",
        "        for layer in [self.resnet.layer3, self.resnet.layer4]:\n",
        "            for module in layer.modules():\n",
        "                if isinstance(module, nn.Conv2d) and module.stride == (2, 2):\n",
        "                    module.stride = (1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input size for reference\n",
        "        input_size = x.shape[2:]\n",
        "\n",
        "        # ResNet layers\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        x_4 = self.resnet.layer1(x)     # 1/4, 64 channels\n",
        "        x_8 = self.resnet.layer2(x_4)   # 1/8, 128 channels\n",
        "        x_16 = self.resnet.layer3(x_8)  # 1/8, 256 channels (stride modified)\n",
        "        x_32 = self.resnet.layer4(x_16) # 1/8, 512 channels (stride modified)\n",
        "\n",
        "        # Upsample if needed (to 1/4)\n",
        "        if self.if_upsample_2x:\n",
        "            x = F.interpolate(x_32, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            x = x_32\n",
        "\n",
        "        # Output convolution to reduce channels\n",
        "        x = self.conv_pred(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "susdjwokL-79"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerConv2d(nn.Sequential):\n",
        "    \"\"\"Two-layer convolutional module used in the prediction head\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TwoLayerConv2d, self).__init__(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
        "        )"
      ],
      "metadata": {
        "id": "i4_dKZbI3DeV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model Implementation"
      ],
      "metadata": {
        "id": "HD3EMiia2h0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseChangeDetectionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base model for change detection consisting of:\n",
        "    1. ResNet18 backbone\n",
        "    2. Feature differencing\n",
        "    3. Prediction head\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2, output_channels=32):\n",
        "        super(BaseChangeDetectionModel, self).__init__()\n",
        "\n",
        "        # ResNet18 backbone\n",
        "        self.backbone = ResNetBackbone(output_channels=output_channels)\n",
        "\n",
        "        # Prediction head - a small CNN for change discrimination\n",
        "        self.classifier = TwoLayerConv2d(output_channels, num_classes)\n",
        "\n",
        "        # Whether to upsample by 2x in the backbone\n",
        "        self.if_upsample_2x = True\n",
        "\n",
        "        # Whether to apply sigmoid at output\n",
        "        self.output_sigmoid = False\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x1: Image at time 1 (B, C, H, W)\n",
        "            x2: Image at time 2 (B, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            change_pred: Change prediction (B, num_classes, H, W)\n",
        "        \"\"\"\n",
        "        # Extract features from both temporal images\n",
        "        feat1 = self.forward_single(x1)\n",
        "        feat2 = self.forward_single(x2)\n",
        "\n",
        "        # Feature differencing\n",
        "        diff = torch.abs(feat1 - feat2)\n",
        "\n",
        "        # Final upsampling to original resolution\n",
        "        if not self.if_upsample_2x:\n",
        "            diff = F.interpolate(diff, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        diff = F.interpolate(diff, scale_factor=4, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Classification head\n",
        "        change_pred = self.classifier(diff)\n",
        "\n",
        "        # Apply sigmoid if needed (for binary case)\n",
        "        if self.output_sigmoid:\n",
        "            change_pred = self.sigmoid(change_pred)\n",
        "\n",
        "        # Ensure output size matches the input size\n",
        "        original_size = x1.shape[2:]\n",
        "        if change_pred.shape[2:] != original_size:\n",
        "            change_pred = F.interpolate(change_pred, size=original_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return change_pred\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        \"\"\"Forward pass for a single temporal image\"\"\"\n",
        "        return self.backbone(x)"
      ],
      "metadata": {
        "id": "A_enRV0L1bEX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n"
      ],
      "metadata": {
        "id": "IAGFEahZ3YH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for change detection\n",
        "class ChangeDetectionDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', img_size=256, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Root directory of the dataset\n",
        "            split: 'train', 'val', or 'test'\n",
        "            img_size: Size to resize images to\n",
        "            transform: Optional transform to apply to the images\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "        # Define directories\n",
        "        self.img_dir_A = os.path.join(root_dir, 'A')  # Images from time 1\n",
        "        self.img_dir_B = os.path.join(root_dir, 'B')  # Images from time 2\n",
        "        self.label_dir = os.path.join(root_dir, 'label')  # Change masks\n",
        "\n",
        "        # Read the list file to get image names\n",
        "        list_file = os.path.join(root_dir, 'list', f'{split}.txt')\n",
        "        with open(list_file, 'r') as f:\n",
        "            self.img_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        print(f\"Loaded {len(self.img_names)} images for {split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_names[idx]\n",
        "\n",
        "        # Load images and label\n",
        "        img_A_path = os.path.join(self.img_dir_A, img_name)\n",
        "        img_B_path = os.path.join(self.img_dir_B, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name)\n",
        "\n",
        "        img_A = Image.open(img_A_path).convert('RGB')\n",
        "        img_B = Image.open(img_B_path).convert('RGB')\n",
        "        label = Image.open(label_path).convert('L')  # Grayscale\n",
        "\n",
        "        # Resize images\n",
        "        if img_A.size[0] != self.img_size or img_A.size[1] != self.img_size:\n",
        "            img_A = img_A.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "            img_B = img_B.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "            label = label.resize((self.img_size, self.img_size), Image.NEAREST)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        img_A = np.array(img_A)\n",
        "        img_B = np.array(img_B)\n",
        "        label = np.array(label)\n",
        "\n",
        "        # Normalize label to 0/1\n",
        "        label = label // 255\n",
        "\n",
        "        # Apply transforms if specified\n",
        "        if self.transform:\n",
        "            img_A = self.transform(img_A)\n",
        "            img_B = self.transform(img_B)\n",
        "        else:\n",
        "            # Default normalization\n",
        "            img_A = img_A.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
        "            img_B = img_B.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
        "\n",
        "            # Normalize with ImageNet mean and std\n",
        "            mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "            std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "            img_A = (img_A - mean) / std\n",
        "            img_B = (img_B - mean) / std\n",
        "\n",
        "            # Convert to tensor with explicit float32 type\n",
        "            img_A = torch.from_numpy(img_A).float()\n",
        "            img_B = torch.from_numpy(img_B).float()\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.from_numpy(label).long().unsqueeze(0)\n",
        "\n",
        "        return {\n",
        "            'A': img_A,\n",
        "            'B': img_B,\n",
        "            'L': label,\n",
        "            'name': img_name\n",
        "        }"
      ],
      "metadata": {
        "id": "e0ygRRrg3ajw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_sample_dataset():\n",
        "    # Check if the data is already downloaded\n",
        "    if os.path.exists('samples/A') and os.path.exists('samples/B') and os.path.exists('samples/label'):\n",
        "        print(\"Sample dataset already exists.\")\n",
        "        return\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs('samples/A', exist_ok=True)\n",
        "    os.makedirs('samples/B', exist_ok=True)\n",
        "    os.makedirs('samples/label', exist_ok=True)\n",
        "    os.makedirs('samples/list', exist_ok=True)\n",
        "\n",
        "    print(\"Creating a toy dataset for demonstration...\")\n",
        "\n",
        "    # Create sample list files\n",
        "    with open('samples/list/train.txt', 'w') as f:\n",
        "        f.write(\"sample1.png\\nsample2.png\\n\")\n",
        "\n",
        "    with open('samples/list/val.txt', 'w') as f:\n",
        "        f.write(\"sample3.png\\n\")\n",
        "\n",
        "    # Create synthetic images with changes\n",
        "    for i, name in enumerate(['sample1.png', 'sample2.png', 'sample3.png']):\n",
        "        # Base image (random noise)\n",
        "        np.random.seed(i)\n",
        "        img_base = np.random.randint(100, 200, (256, 256, 3), dtype=np.uint8)\n",
        "\n",
        "        # Create time 1 image - add some structures\n",
        "        img_A = img_base.copy()\n",
        "\n",
        "        # Add rectangles of different colors at different positions\n",
        "        for j in range(3):\n",
        "            x1, y1 = np.random.randint(50, 150, 2)\n",
        "            x2, y2 = x1 + np.random.randint(30, 80), y1 + np.random.randint(30, 80)\n",
        "            color = np.random.randint(0, 255, 3)\n",
        "            img_A[y1:y2, x1:x2] = color\n",
        "\n",
        "        # Create time 2 image - keep some structures, change others\n",
        "        img_B = img_base.copy()\n",
        "\n",
        "        # Add the same rectangles but change some of them\n",
        "        change_mask = np.zeros((256, 256), dtype=np.uint8)\n",
        "\n",
        "        for j in range(3):\n",
        "            x1, y1 = np.random.randint(50, 150, 2)\n",
        "            x2, y2 = x1 + np.random.randint(30, 80), y1 + np.random.randint(30, 80)\n",
        "            color = np.random.randint(0, 255, 3)\n",
        "            img_B[y1:y2, x1:x2] = color\n",
        "\n",
        "            # Mark some rectangles as changed\n",
        "            if j % 2 == 0:\n",
        "                change_mask[y1:y2, x1:x2] = 255\n",
        "\n",
        "        # Save images\n",
        "        Image.fromarray(img_A).save(f'samples/A/{name}')\n",
        "        Image.fromarray(img_B).save(f'samples/B/{name}')\n",
        "        Image.fromarray(change_mask).save(f'samples/label/{name}')\n",
        "\n",
        "    print(\"Toy dataset created with synthetic changes.\")"
      ],
      "metadata": {
        "id": "wquSSlVm3g-u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_levir_dataset(drive_path, save_dir='LEVIR-s2looking10_dataset'):\n",
        "    \"\"\"\n",
        "    Set up the dataset using files from Google Drive\n",
        "\n",
        "    Args:\n",
        "        drive_path: Path to the folder on Google Drive containing A, B, and label folders\n",
        "        save_dir: Local directory to save the dataset structure\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "\n",
        "    # Create local directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, 'list'), exist_ok=True)\n",
        "\n",
        "    # Check if the source directories exist\n",
        "    a_dir = os.path.join(drive_path, 'A')\n",
        "    b_dir = os.path.join(drive_path, 'B')\n",
        "    label_dir = os.path.join(drive_path, 'label')\n",
        "\n",
        "    if not (os.path.exists(a_dir) and os.path.exists(b_dir) and os.path.exists(label_dir)):\n",
        "        print(f\"Error: Required directories not found in {drive_path}\")\n",
        "        print(f\"Expected structure: {drive_path}/A, {drive_path}/B, {drive_path}/label\")\n",
        "        return None\n",
        "\n",
        "    # Create symbolic links to avoid copying large files\n",
        "    os.symlink(a_dir, os.path.join(save_dir, 'A'))\n",
        "    os.symlink(b_dir, os.path.join(save_dir, 'B'))\n",
        "    os.symlink(label_dir, os.path.join(save_dir, 'label'))\n",
        "\n",
        "    # Get all image filenames\n",
        "    all_files = [f for f in os.listdir(a_dir) if f.endswith('.png')]\n",
        "    all_files.sort()\n",
        "\n",
        "    print(f\"Found {len(all_files)} images in the dataset\")\n",
        "\n",
        "    # Split into train/val (80/20 split)\n",
        "    train_size = int(len(all_files) * 0.8)\n",
        "    train_files = all_files[:train_size]\n",
        "    val_files = all_files[train_size:]\n",
        "\n",
        "    # Create list files\n",
        "    with open(os.path.join(save_dir, 'list', 'train.txt'), 'w') as f:\n",
        "        for filename in train_files:\n",
        "            f.write(f\"{filename}\\n\")\n",
        "\n",
        "    with open(os.path.join(save_dir, 'list', 'val.txt'), 'w') as f:\n",
        "        for filename in val_files:\n",
        "            f.write(f\"{filename}\\n\")\n",
        "\n",
        "    print(f\"Created dataset with {len(train_files)} training and {len(val_files)} validation images\")\n",
        "    return save_dir"
      ],
      "metadata": {
        "id": "y7oCjshb3mjE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "AxgKDoq03sQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_one_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(data_loader):\n",
        "        # Get data\n",
        "        img_A = batch['A'].to(device)\n",
        "        img_B = batch['B'].to(device)\n",
        "        labels = batch['L'].to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(img_A, img_B)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels.squeeze(1))\n",
        "\n",
        "        # Backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(data_loader)\n"
      ],
      "metadata": {
        "id": "yFyWmy0I3tUl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Validation function\n",
        "def validate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader):\n",
        "            # Get data\n",
        "            img_A = batch['A'].to(device)\n",
        "            img_B = batch['B'].to(device)\n",
        "            labels = batch['L'].to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(img_A, img_B)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels.squeeze(1))\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0) * labels.size(2) * labels.size(3)\n",
        "            correct += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(data_loader)\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    return val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "tnYhnxcp3vur"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions\n"
      ],
      "metadata": {
        "id": "kfE0SPYP3z0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prediction visualization\n",
        "def visualize_predictions(model, data_loader, device, num_samples=3):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5*num_samples))\n",
        "\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            # Get data\n",
        "            img_A = batch['A'].to(device)\n",
        "            img_B = batch['B'].to(device)\n",
        "            labels = batch['L']\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(img_A, img_B)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted = predicted.cpu().numpy()\n",
        "\n",
        "            # Convert images back to numpy for visualization\n",
        "            img_A = img_A[0].cpu().numpy().transpose(1, 2, 0)\n",
        "            img_B = img_B[0].cpu().numpy().transpose(1, 2, 0)\n",
        "            label = labels[0].cpu().numpy().squeeze()\n",
        "\n",
        "            # Denormalize images\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            img_A = std * img_A + mean\n",
        "            img_B = std * img_B + mean\n",
        "            img_A = np.clip(img_A, 0, 1)\n",
        "            img_B = np.clip(img_B, 0, 1)\n",
        "\n",
        "            # Plot\n",
        "            axes[i, 0].imshow(img_A)\n",
        "            axes[i, 0].set_title('Image A (t1)')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            axes[i, 1].imshow(img_B)\n",
        "            axes[i, 1].set_title('Image B (t2)')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            axes[i, 2].imshow(label, cmap='gray')\n",
        "            axes[i, 2].set_title('Ground Truth')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "            axes[i, 3].imshow(predicted[0], cmap='gray')\n",
        "            axes[i, 3].set_title('Prediction')\n",
        "            axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions.png')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sni1PCFA31jV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Set path to your Google Drive folder containing the dataset\n",
        "    # Adjust this path to match your Google Drive folder structure\n",
        "    drive_path = '/content/drive/MyDrive/data/s2Looking10'\n",
        "\n",
        "\n",
        "    # Set up the dataset\n",
        "    dataset_dir = setup_levir_dataset(drive_path)\n",
        "\n",
        "    if dataset_dir is None:\n",
        "        print(\"Failed to set up dataset. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Set hyperparameters\n",
        "    batch_size = 4\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    img_size = 256\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = ChangeDetectionDataset(root_dir=dataset_dir, split='train', img_size=img_size)\n",
        "    val_dataset = ChangeDetectionDataset(root_dir=dataset_dir, split='val', img_size=img_size)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Create model\n",
        "    model = BaseChangeDetectionModel(num_classes=2, output_channels=32).to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Visualize predictions\n",
        "    visualize_predictions(model, val_loader, device)\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'base_model.pth')\n",
        "    print(\"Model saved.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "hLAT0GN9367b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4_EwU-1Dtrd"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}